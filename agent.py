
"""
Main agent - generates custom parsers for bank PDFs

Flow: plan → generate → verify → fix (up to max_iters)

- Creates custom_parsers/<target>_parser.py on first run
- Parser looks at the expected CSV next to the PDF to figure out:
    • what columns we need
    • what data types to use  
    • what date format to match
- Tries pdfplumber table extraction first, falls back to line parsing
- Filters out header/footer junk by looking for date patterns
- Normalizes amounts and dates to match expected format exactly

Has a basic self-fix loop:
- When output doesn't match expected, analyzes the diff and patches the parser
- Does things like reordering columns, fixing data types, dropping empty rows
- Retries until it works or hits max iterations

Kept simple and deterministic, works offline.
"""

from __future__ import annotations
import argparse
import importlib
import importlib.util
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import re

import pandas as pd

# Project structure
ROOT = Path(__file__).resolve().parent
DATA_DIR = ROOT / "data"
PARSERS_DIR = ROOT / "custom_parsers"
PARSERS_DIR.mkdir(parents=True, exist_ok=True)
(PARSERS_DIR / "__init__.py").touch(exist_ok=True)

DEBUG_DIR = ROOT / "debug"
DEBUG_DIR.mkdir(parents=True, exist_ok=True)

# Helper functions
def log(msg: str) -> None:
    print(f"[agent] {msg}", flush=True)

def _first_existing(paths: List[Path]) -> Optional[Path]:
    for p in paths:
        if p.exists():
            return p
    return None

def load_expected_csv(target: str) -> Tuple[Path, pd.DataFrame]:
    """Find and load data/<target>/result.csv (or <target>.csv)"""
    tdir = DATA_DIR / target
    candidates = [tdir / "result.csv", tdir / f"{target}.csv"]
    c = _first_existing(candidates)
    if not c:
        raise FileNotFoundError(
            f"Expected CSV not found. Looked for: {', '.join(map(str, candidates))}"
        )
    exp = pd.read_csv(c)
    return c, exp

def locate_pdf(target: str) -> Path:
    """Find first *.pdf in data/<target>/ - prefer one with target name"""
    tdir = DATA_DIR / target
    pdfs = sorted(tdir.glob("*.pdf"))
    if not pdfs:
        raise FileNotFoundError(f"No PDF found under {tdir}")
    pref = [p for p in pdfs if target.lower() in p.stem.lower()]
    return pref[0] if pref else pdfs[0]

def pretty_diff(got: pd.DataFrame, exp: pd.DataFrame, max_rows: int = 6) -> str:
    msgs: List[str] = []
    if list(got.columns) != list(exp.columns):
        msgs.append(
            "Column schema mismatch.\n"
            f"  got: {list(got.columns)}\n"
            f"  exp: {list(exp.columns)}"
        )
        return "\n".join(msgs)

    if len(got) != len(exp):
        msgs.append(f"Row count mismatch: got {len(got)} vs exp {len(exp)}")

    diffs = []
    for i in range(min(len(got), len(exp))):
        rg = got.iloc[i]
        rexp = exp.iloc[i]
        for c in got.columns:
            g, e = rg[c], rexp[c]
            # Check if equal including NaN handling
            if (pd.isna(g) and pd.isna(e)) or (g == e):
                continue
            diffs.append((i, c, g, e))
            if len(diffs) >= max_rows:
                break
        if len(diffs) >= max_rows:
            break

    if diffs:
        msgs.append("First diffs (row, col, got, exp):")
        for (i, c, g, e) in diffs:
            msgs.append(f"  ({i}, {c!r}): {g!r} != {e!r}")

    return "\n".join(msgs) if msgs else "no visible diffs (but DataFrame.equals returned False)"

# Code generation - write the actual parser
def write_parser_file(target: str) -> Path:
    """
    Generate a parser that imports helpers from base_parser.
    Keeps the generated file clean and readable. Has thin wrappers for 
    backwards compatibility with existing tests.
    """
    path = PARSERS_DIR / f"{target}_parser.py"

    code = r'''# Auto-generated by agent.py for target="__TARGET__"
from __future__ import annotations
from pathlib import Path
import pandas as pd
import numpy as np

# Import shared helpers (keeps this file small)
try:
    from .base_parser import (
        DATE_PATTERNS,
        maybe_date,
        normalize_amount,
        clean_desc,
        coerce_to_expected_date_format,
        extract_tables_pdfplumber,
        extract_lines_pdf,
        build_df_from_tables,
        build_df_from_lines,
    )
except Exception:
    from custom_parsers.base_parser import (
        DATE_PATTERNS,
        maybe_date,
        normalize_amount,
        clean_desc,
        coerce_to_expected_date_format,
        extract_tables_pdfplumber,
        extract_lines_pdf,
        build_df_from_tables,
        build_df_from_lines,
    )

# Backwards compatibility wrappers for tests/monkeypatching
def _extract_tables_pdfplumber(pdf_path: Path):
    return extract_tables_pdfplumber(pdf_path)

def _extract_lines(pdf_path: Path):
    return extract_lines_pdf(pdf_path)

# Main parsing function
def parse(pdf_path: str) -> pd.DataFrame:
    pdf = Path(pdf_path)
    bank = pdf.parent.name

    # Load expected CSV (result.csv or <bank>.csv)
    exp = None
    for c in [pdf.parent / "result.csv", pdf.parent / f"{bank}.csv"]:
        if c.exists():
            exp = pd.read_csv(c)
            break
    if exp is None:
        raise FileNotFoundError("Expected CSV not found next to PDF")

    expected_columns = list(exp.columns)
    expected_dtypes = exp.dtypes.to_dict()

    # Try table extraction first, then fall back to line parsing
    tables = _extract_tables_pdfplumber(pdf)
    df = build_df_from_tables(tables, expected_columns)
    if df is None or df.empty:
        lines = _extract_lines(pdf)
        df = build_df_from_lines(lines, expected_columns)

    # Filter out header/footer rows by keeping only rows with dates
    date_col = expected_columns[0]
    df = df[df[date_col].astype(str).str.match("|".join(DATE_PATTERNS), na=False)]

    # --- [PATCHPOINT:REINDEX] ---
    df = df.reindex(columns=expected_columns)
    # --- [/PATCHPOINT:REINDEX] ---

    # Make date format match expected CSV
    if date_col in expected_dtypes:
        df[date_col] = coerce_to_expected_date_format(df[date_col], exp[date_col])

    # Cast to expected types for exact matching
    for col in df.columns:
        if col in expected_dtypes and "float" in str(expected_dtypes[col]):
            # --- [PATCHPOINT:NUMERIC_CAST] ---
            df[col] = df[col].map(normalize_amount).astype("float64")
            # --- [/PATCHPOINT:NUMERIC_CAST] ---
        else:
            df[col] = df[col].astype("object").astype(str).str.strip()

    # --- [PATCHPOINT:CLEANUP] ---
    df = df.reset_index(drop=True)
    return df
    # --- [/PATCHPOINT:CLEANUP] ---
'''
    code = code.replace("__TARGET__", target)
    path.write_text(code, encoding="utf-8")
    return path

# Verification - check if our output matches expected
def _dynamic_import(module_path: Path, module_name: str):
    if module_name in sys.modules:
        del sys.modules[module_name]
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    if spec is None or spec.loader is None:
        raise ImportError(f"Unable to import {module_name}")
    module = importlib.util.module_from_spec(spec)
    sys.modules[module_name] = module
    spec.loader.exec_module(module)  # type: ignore
    return module

def compare_with_expected(target: str, verbose: bool = True) -> Tuple[bool, pd.DataFrame, pd.DataFrame, str]:
    csv_path, exp = load_expected_csv(target)
    pdf_path = locate_pdf(target)

    # Import the generated parser dynamically
    mod_name = f"custom_parsers.{target}_parser"
    module = _dynamic_import(PARSERS_DIR / f"{target}_parser.py", mod_name)

    if not hasattr(module, "parse"):
        raise AttributeError(f"{mod_name} has no function parse(pdf_path: str) -> pd.DataFrame")

    got: pd.DataFrame = module.parse(str(pdf_path))

    equal_schema = list(got.columns) == list(exp.columns)
    equals = got.reset_index(drop=True).equals(exp.reset_index(drop=True))

    if verbose:
        log(f"Verified against: {csv_path.name}")
        log(f"PDF: {pdf_path.name}")
        log(f"Schema equal? {equal_schema}")
        log(f"Strict equals? {equals}")

    diff_text = ""
    if not equals:
        got_path = DEBUG_DIR / f"{target}_got.csv"
        exp_path = DEBUG_DIR / f"{target}_expected.csv"
        got.to_csv(got_path, index=False)
        exp.to_csv(exp_path, index=False)
        diff_text = pretty_diff(got, exp)
        log("Mismatch details:\n" + diff_text)
        log(f"Wrote debug CSVs:\n  {got_path}\n  {exp_path}")

    return (equal_schema and equals), got, exp, diff_text

# Self-fix logic - patch the parser based on what went wrong
def refine_parser_code(prev_code: str, diff_text: str) -> str:
    """
    Try to fix common issues automatically.
    - Uses [PATCHPOINT:*] markers in generated code for clean patching
    - Falls back to string replacement for legacy code without markers
    """
    patched = prev_code
    has_markers = "[PATCHPOINT:" in prev_code

    # New approach: patch specific blocks marked with [PATCHPOINT:*]
    if has_markers:
        def _patch_block(src: str, name: str, new_body_lines: list[str]) -> str:
            pattern = re.compile(
                rf"(# --- \[PATCHPOINT:{name}\] ---\n)(.*?)(\n\s*# --- \[/PATCHPOINT:{name}\] ---)",
                flags=re.DOTALL,
            )
            def repl(m):
                original = m.group(2)
                first_line = original.splitlines()[0] if original.splitlines() else ""
                leading_ws = re.match(r"^(\s*)", first_line).group(1) if first_line else ""
                body = "\n".join(leading_ws + ln for ln in new_body_lines)
                return m.group(1) + body + m.group(3)
            return pattern.sub(repl, src, count=1)

        # Fix row count issues
        if "Row count mismatch" in diff_text:
            patched = _patch_block(
                patched, "CLEANUP",
                [
                    "df = df.dropna(how='all')",
                    "df = df.reset_index(drop=True)",
                    "return df",
                ],
            )
        # Fix numeric precision issues
        if "First diffs" in diff_text:
            patched = _patch_block(
                patched, "NUMERIC_CAST",
                [
                    "df[col] = df[col].map(normalize_amount)",
                    "df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')",
                ],
            )
        # Schema reordering is already handled by REINDEX block
        return patched

    # Legacy fallback for code without markers
    # Fix column ordering
    if "Column schema mismatch" in diff_text and "reindex(columns=expected_columns)" not in patched:
        if "df = df.reset_index(drop=True)\n    return df" in patched:
            patched = patched.replace(
                "df = df.reset_index(drop=True)\n    return df",
                "df = df.reindex(columns=expected_columns)\n"
                "    df = df.reset_index(drop=True)\n"
                "    return df",
                1,
            )

    # Fix extra empty rows
    if "Row count mismatch" in diff_text and "dropna(how='all')" not in patched:
        if "df = df.reset_index(drop=True)\n    return df" in patched:
            patched = patched.replace(
                "df = df.reset_index(drop=True)\n    return df",
                "df = df.dropna(how='all')\n"
                "    df = df.reset_index(drop=True)\n"
                "    return df",
                1,
            )

    # Better numeric conversion
    if "First diffs" in diff_text and "pd.to_numeric" not in patched:
        patched = patched.replace(
            'df[col] = df[col].map(normalize_amount).astype("float64")',
            "df[col] = df[col].map(normalize_amount)\n"
            "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')",
        )

    return patched

# Main agent loop
@dataclass
class AgentConfig:
    target: str
    max_iters: int = 3
    verbose: bool = True

def run_agent(cfg: AgentConfig) -> int:
    """
    Simple loop:
      1) Make sure parser exists (generate if needed)
      2) Test it; if it doesn't match, try to fix it (up to max_iters)
    """
    target = cfg.target.lower().strip()
    parser_file = PARSERS_DIR / f"{target}_parser.py"

    for attempt in range(1, cfg.max_iters + 1):
        log(f"Attempt {attempt}/{cfg.max_iters}")

        # Generate parser if it doesn't exist
        if not parser_file.exists():
            write_parser_file(target)
            log(f"Generated parser: {parser_file}")

        # Test it
        ok, got, exp, diff_text = compare_with_expected(target, verbose=cfg.verbose)
        if ok:
            log("✅ Success: Parser output matches expected CSV exactly.")
            return 0

        # Try to fix it based on what went wrong
        try:
            code = parser_file.read_text(encoding="utf-8")
        except Exception as e:
            log(f"Failed to read parser file for refinement: {e}")
            return 1

        new_code = refine_parser_code(code, diff_text or "")
        if new_code != code:
            parser_file.write_text(new_code, encoding="utf-8")
            log("Applied fixes based on diff. Retrying...")
        else:
            # No fix worked, try regenerating from scratch
            write_parser_file(target)
            log("No fixes applied; regenerated parser. Retrying...")

    log("❌ Failed to get exact match within max iterations.")
    return 1

# Command line interface
def main(argv: Optional[List[str]] = None) -> int:
    ap = argparse.ArgumentParser(description="Agent-as-Coder: bank statement parser generator")
    ap.add_argument("--target", required=True, help="bank key under data/, e.g., icici")
    ap.add_argument("--max-iters", type=int, default=3, help="max self-fix attempts")
    ap.add_argument("--quiet", action="store_true", help="suppress verbose logs")
    args = ap.parse_args(argv)

    cfg = AgentConfig(target=args.target, max_iters=args.max_iters, verbose=not args.quiet)
    return run_agent(cfg)

if __name__ == "__main__":
    sys.exit(main())